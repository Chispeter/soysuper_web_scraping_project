# Proyecto de Web Scraping de Soysuper ğŸ›’
## Â¿En quÃ© consiste? ğŸ“
Este proyecto consiste en hacer web scraping de la pÃ¡gina web [Soysuper](https://soysuper.com/), un supermercado online que te permite comparar precios de diferentes productos en distintos supermercados.

## Â¿CuÃ¡l es la motivaciÃ³n? ğŸ’ª
La motivaciÃ³n de este proyecto es explorar y manejar distintas librerÃ­as como forma de crecimiento profesional. AdemÃ¡s, Â¿a quiÃ©n no le va a gustar extraer buenos datos?

## Â¿CuÃ¡l es el objetivo? ğŸ¯
El objetivo es extraer los datos de los productos (como el nombre, la marca o el precio) que aparecen en la web y guardarlos en archivos JSON.

## Â¿CÃ³mo funciona? ğŸ› ï¸
El proyecto usa la librerÃ­a [requests](https://pypi.org/project/requests/) para hacer peticiones HTTP a la web de Soysuper, y la librerÃ­a [BeautifulSoup](https://pypi.org/project/beautifulsoup4/) para parsear el HTML y extraer los datos de interÃ©s. El proyecto estÃ¡ dividido en varios mÃ³dulos:
- **```config.py```:** Contiene una funciÃ³n que lee el archivo de configuraciÃ³n ```config.json``` y devuelve los parÃ¡metros del proyecto, como el nombre del host de la pÃ¡gina web y el nombre del directorio en el que se van a guardar los datos.
- **```web_scraping.py```:** Contiene diferentes funciones que extraen los datos de categorÃ­as, subcategorÃ­as, productos y supermercados de la web. Los datos extraÃ­dos se guardan en archivos JSON con ayuda del mÃ³dulo auxiliar ```utils.py```.
- **```utils.py```:** Contiene funciones auxiliares para (1) crear una serie de directorios por cada categorÃ­a, (2) escribir los achivos JSON, (3) limpiar cadenas de caracteres y (4) comprobar los Ãºltimos datos extraÃ­dos.
- **```main.py```:** Contiene el cÃ³digo principal que ejecuta el web scraping.

## Â¿CÃ³mo se utiza? âš™ï¸
Para usar este proyecto, se necesita tener instalado python 3 y las librerÃ­as requests y BeautifulSoup. Los pasos son los siguientes:
1. Clonar o descargar este repositorio.
2. Crear y activar un entorno virtual (opcional). Se recomienda usar un entorno virtual para instalar las dependencias.
3. Instalar las dependencias con ```pip install -r requirements.txt```.
4. Ejecutar el archivo ```main.py``` con ```python main.py```.
5. Esperar a que termine el web scraping y revisar los archivos JSON generados en el directorio ```data```.
ğŸ”¥NOVEDADğŸ”¥En caso de detener la ejecuciÃ³n de ```main.py```, la prÃ³xima vez que se vuelva a ejecutar, el mÃ³dulo ```web_scraping.py``` comprobarÃ¡ los Ãºltimos datos extraÃ­dos y continuarÃ¡ por la Ãºltima categorÃ­a registrada en el directorio ```data```.

## Advertencia âš ï¸
Este proyecto tiene fines educativos y de entretenimiento. No se pretende violar los tÃ©rminos y condiciones de Soysuper, ni hacer un uso indebido de sus datos. Se recomienda hacer un uso responsable y moderado de este proyecto, respetando los derechos de autor y la privacidad de Soysuper y de los supermercados que compara. El autor no se hace responsable de las consecuencias que pueda tener el uso de este proyecto.

## Agradecimientos ğŸ«¶
Quiero agradecer a Soysuper por ofrecer un servicio tan Ãºtil y prÃ¡ctico para los consumidores online, y por tener una web tan bien estructurada y fÃ¡cil de hacer web scraping. TambiÃ©n quiero agradecer a Microsoft Copilot por ayudarme a redactar este README.md, y por ser un buscador tan inteligente y amigable. Y por Ãºltimo, quiero agradecer a ti, por leer este README.md y por interesarte por este proyecto. Espero que te haya gustado y que lo disfrutes. Â¡Hasta pronto! ğŸ™Œ

