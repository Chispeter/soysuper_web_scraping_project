# Proyecto de Web Scraping de Soysuper üõí
## ¬øEn qu√© consiste? üìù
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Este proyecto consiste en hacer web scraping de la p√°gina web [Soysuper](https://soysuper.com/), un supermercado online que te permite comparar precios de diferentes productos en distintos supermercados.

## ¬øCu√°l es la motivaci√≥n? üí™
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; La motivaci√≥n de este proyecto es (1) explorar y manejar distintas librer√≠as como modo de entretenimiento y (2) conocer y poner en pr√°ctica una peque√±a parte de la etapa de extracci√≥n en los procesos ETL y ELT como forma de crecimiento profesional.

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Adem√°s, ¬øa qui√©n no le va a gustar una buena extracci√≥n de datos?

## ¬øCu√°l es el objetivo? üéØ
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; El objetivo es extraer los datos de los productos (como el nombre, la marca y el precio) que aparecen en la web y guardarlos en archivos JSON.

## ¬øC√≥mo funciona? üõ†Ô∏è
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; El proyecto usa la librer√≠a [requests](https://pypi.org/project/requests/) para hacer peticiones HTTP a la p√°gina web de [Soysuper](https://soysuper.com/), y la librer√≠a [BeautifulSoup](https://pypi.org/project/beautifulsoup4/) para parsear el HTML y extraer los datos de inter√©s para, posteriormente, guardarlos en archivos JSON en diferentes directorios con ayuda de las librer√≠as json y os. El proyecto est√° dividido en varios m√≥dulos:

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ‚Ä¢ **`config.py:`** Contiene una funci√≥n que lee el archivo de configuraci√≥n `config.json` y devuelve los par√°metros del proyecto, como el nombre del host de la p√°gina web y el nombre del directorio en el que se van a guardar los datos.

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ‚Ä¢ **`web_scraping.py`:** Contiene diferentes funciones que extraen los datos de categor√≠as, subcategor√≠as, productos y supermercados de la p√°gina web. Los datos extra√≠dos se guardan en archivos JSON con ayuda del m√≥dulo auxiliar `utils.py`.

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ‚Ä¢ **`utils.py`:** Contiene funciones auxiliares para (1) crear una serie de directorios por cada categor√≠a, (2) escribir los achivos JSON, (3) limpiar cadenas de caracteres y (4) comprobar los √∫ltimos datos extra√≠dos.

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ‚Ä¢ **`main.py`:** Contiene el c√≥digo principal que ejecuta el web scraping.

## ¬øC√≥mo se utiza? ‚öôÔ∏è
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Para usar este proyecto, se necesita tener instalado [Python 3](https://www.python.org/downloads/) y las librer√≠as [requests](https://pypi.org/project/requests/) y [BeautifulSoup](https://pypi.org/project/beautifulsoup4/). Los pasos son los siguientes:

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; **1.** Clonar o descargar este repositorio.

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; **2.** Crear y activar un entorno virtual (opcional). Se recomienda usar un entorno virtual para instalar las dependencias.

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; **3.** Instalar las dependencias con `pip install -r requirements.txt`.

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; **4.** Ejecutar el archivo `main.py` con `python main.py`.

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; **5.** Esperar a que termine el web scraping y revisar los archivos JSON generados en el directorio `data`, creado por defecto.

üî•**NOVEDAD**üî•En caso de detener la ejecuci√≥n de `main.py`, la pr√≥xima vez que se vuelva a ejecutar, el m√≥dulo `web_scraping.py` comprobar√° los √∫ltimos datos extra√≠dos y continuar√° por la √∫ltima categor√≠a registrada en el directorio `data`.

## Advertencia ‚ö†Ô∏è
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Este proyecto tiene fines educativos y de entretenimiento. No se pretende violar los t√©rminos y condiciones de Soysuper, ni hacer un uso indebido de sus datos. Se recomienda hacer un uso responsable y moderado de este proyecto, respetando los derechos de autor y la privacidad de Soysuper y de los supermercados que compara. El autor no se hace responsable de las consecuencias que pueda tener el uso de este proyecto.

## Agradecimientos ü´∂
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Quiero agradecer a Soysuper por ofrecer un servicio tan √∫til y pr√°ctico para los consumidores online, y por tener una p√°gina web tan bien estructurada y f√°cil de hacer web scraping. Tambi√©n quiero agradecer a Bing por ayudarme a redactar este README.md, y por ser un buscador tan inteligente y amigable. Y por √∫ltimo, quiero agradecer a ti, por leer este README.md y por interesarte por este proyecto. Espero que te haya gustado y que lo disfrutes. ¬°Hasta pronto! üôå

